{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReviewSentimentRF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSww1cS1kLv0",
        "outputId": "9b2a8516-8378-4c97-acf8-9d0a731009c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |▊                               | 30 kB 40.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 41.9 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51 kB 43.4 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61 kB 47.3 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71 kB 33.7 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 92 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 102 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 112 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 122 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 133 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 143 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 153 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 163 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 174 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 184 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 194 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 204 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████                           | 215 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 225 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 235 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 245 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 256 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 266 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 276 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 286 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 296 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 307 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 317 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 327 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 337 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 348 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 358 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 368 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 378 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 389 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 399 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 409 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 419 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 430 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 440 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 450 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 460 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 471 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 481 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 491 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 501 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 512 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 522 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 532 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 542 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 552 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 563 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 573 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 583 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 593 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 604 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 614 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 624 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 634 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 645 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 655 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 665 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 675 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 686 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 696 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 706 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 716 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 727 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 737 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 747 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 757 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 768 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 778 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 788 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 798 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 808 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 819 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 829 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 839 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 849 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 860 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 870 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 880 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 890 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 901 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 911 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 921 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 931 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 942 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 952 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 962 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 972 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 983 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 993 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.0 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.0 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.0 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.0 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.1 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.1 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.1 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.1 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.1 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.2 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.2 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.2 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.3 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.3 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.3 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.3 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.3 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.3 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.3 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4 MB 33.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4 MB 33.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3) (1.15.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394487 sha256=f7ded05ed19ff53a35721af681732993916e9b6b8d5c1b38b3cdf428cc90d07d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.3\n"
          ]
        }
      ],
      "source": [
        "pip install nltk==3.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "rabKuHaSElxq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples"
      ],
      "metadata": {
        "id": "NspOa4CGkdJd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n"
      ],
      "metadata": {
        "id": "QRFDgUyGQxUH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo8B_MvM43OX",
        "outputId": "ba836914-a1aa-4859-8274-64e1978751a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "text = twitter_samples.strings('tweets.20150430-223406.json')"
      ],
      "metadata": {
        "id": "Qya3xUrsk0Q7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afh_96SGk-C5",
        "outputId": "b7ee838b-ef5c-4327-e004-bdc7f2376b1f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, string\n",
        "\n",
        "def remove_noise(rev_tokens, stop_words = ()):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(rev_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "FpZE5bSEmTgw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "Vnb0GjlhLVNp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCqI_6UHnAnl",
        "outputId": "97373dca-571f-482e-dde6-195e912fc109"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stop_words.append(\"rude\")"
      ],
      "metadata": {
        "id": "wAMCPgsMcLPV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "neg_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "\n",
        "positive_cleaned_tokens_list = []\n",
        "negative_cleaned_tokens_list = []\n",
        "\n",
        "for tokens in pos_tweet_tokens:\n",
        "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "for tokens in neg_tweet_tokens:\n",
        "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
      ],
      "metadata": {
        "id": "CpL5Vcg1lBiU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def lemmatize_sentence(tokens):\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     lemmatized_sentence = []\n",
        "#     for word, tag in pos_tag(tokens):\n",
        "#         if tag.startswith('NN'):\n",
        "#             pos = 'n'\n",
        "#         elif tag.startswith('VB'):\n",
        "#             pos = 'v'\n",
        "#         else:\n",
        "#             pos = 'a'\n",
        "#         lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
        "#     return lemmatized_sentence"
      ],
      "metadata": {
        "id": "qg-KgyBol7Fe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def read_file_pos_neg(file):\n",
        "  df = pd.read_csv(file)\n",
        "  df_list = df.values.tolist()\n",
        "\n",
        "  pos = []\n",
        "  neg = []\n",
        "\n",
        "  for i in df_list:\n",
        "    if i[0] == 'pos':\n",
        "      pos.append(i[1])\n",
        "    if i[0] == 'neg':\n",
        "      neg.append(i[1])\n",
        "    \n",
        "  return pos, neg\n"
      ],
      "metadata": {
        "id": "AHdTO7R3WQKO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "f77qeqw9JjVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb42c18-290b-4c14-94b4-5819e1c3f033"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_s2b, neg_s2b = read_file_pos_neg(\"S2B.csv\")\n",
        "\n",
        "pos_ar, neg_ar = read_file_pos_neg(\"AirlineReviews.csv\")\n",
        "\n",
        "pos_b2s, neg_b2s = read_file_pos_neg(\"B2S.csv\")\n",
        "\n",
        "pos_valid, neg_valid = read_file_pos_neg(\"S2B(N).csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "q-WdJIgUciA_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(pos_s2b)\n",
        "random.shuffle(neg_s2b)\n",
        "random.shuffle(pos_b2s)\n",
        "random.shuffle(neg_b2s)\n",
        "random.shuffle(pos_ar)\n",
        "random.shuffle(neg_ar)\n",
        "random.shuffle(pos_valid)\n",
        "random.shuffle(neg_valid)"
      ],
      "metadata": {
        "id": "Y7nZ2rrneYd0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_b2s = pos_b2s[:1000]"
      ],
      "metadata": {
        "id": "UYxKKibSY6Fn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(neg_s2b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuZ05rZSf7cj",
        "outputId": "aa62eaa9-a8ed-4f6e-d01b-4841ecf0cbfe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "438"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "validation_set = []\n",
        "\n",
        "\n",
        "for i in pos_s2b[5000:]:\n",
        "  validation_set.append([\"P\", i])\n",
        "  \n",
        "\n",
        "for j in neg_s2b[360:]:\n",
        "  validation_set.append([\"N\", j])\n",
        "\n",
        "\n",
        "random.shuffle(validation_set)\n",
        "\n",
        "pos_s2b = pos_s2b[:5000]\n",
        "neg_s2b = neg_s2b[:380]"
      ],
      "metadata": {
        "id": "xEztrWrgUntB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(validation_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGAdOFLOIzdC",
        "outputId": "10b41afa-9955-44d6-91a1-c99d9e208722"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "646"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_list(l):\n",
        "  ls = []\n",
        "  for i in l:\n",
        "    ls.append(remove_noise(word_tokenize(i)))\n",
        "  return ls"
      ],
      "metadata": {
        "id": "OButcoqTtBJX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pis = clean_list(pos_s2b)\n",
        "nis = clean_list(neg_s2b)\n",
        "pib = clean_list(pos_b2s)\n",
        "nib = clean_list(neg_b2s)\n"
      ],
      "metadata": {
        "id": "fMdnAPgAtqlW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_for_model(cleaned_tokens_list):\n",
        "    for rev_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in rev_tokens)\n",
        "\n",
        "positive_tokens_for_model = get_for_model(positive_cleaned_tokens_list[:1000])\n",
        "negative_tokens_for_model = get_for_model(negative_cleaned_tokens_list)\n"
      ],
      "metadata": {
        "id": "bl0KwYLhDzY8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_toks_s2b =  get_for_model(pis)\n",
        "n_toks_s2b = get_for_model(nis)\n",
        "\n",
        "p_toks_b2s =  get_for_model(pib)\n",
        "n_toks_b2s = get_for_model(nib)"
      ],
      "metadata": {
        "id": "Pd9AIlZnL1ti"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "positive_dataset_t = [(rev_dict, \"Positive\")\n",
        "                     for rev_dict in positive_tokens_for_model]\n",
        "\n",
        "negative_dataset_t = [(rev_dict, \"Negative\")\n",
        "                     for rev_dict in negative_tokens_for_model]\n",
        "\n",
        "dataset_t = negative_dataset_t #+ positive_dataset_t "
      ],
      "metadata": {
        "id": "89op80_y1140"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "positive_dataset_r = [(rev_dict, \"Positive\")\n",
        "                     for rev_dict in p_toks_s2b]\n",
        "\n",
        "negative_dataset_r = [(rev_dict, \"Negative\")\n",
        "                     for rev_dict in n_toks_s2b]\n",
        "\n",
        "dataset_r = positive_dataset_r + negative_dataset_r\n",
        "\n",
        "random.shuffle(dataset_r)\n",
        "\n"
      ],
      "metadata": {
        "id": "_4xff4xznKvP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "positive_dataset_a = [(rev_dict, \"Positive\")\n",
        "                     for rev_dict in get_for_model(pos_ar[:1500])]\n",
        "\n",
        "negative_dataset_a = [(rev_dict, \"Negative\")\n",
        "                     for rev_dict in get_for_model(neg_ar[:1500])]\n",
        "\n",
        "dataset_a = positive_dataset_a + negative_dataset_a\n",
        "\n",
        "random.shuffle(dataset_a)"
      ],
      "metadata": {
        "id": "VyxhDHCUisHR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_dataset_b = [(rev_dict, \"Positive\")\n",
        "                     for rev_dict in p_toks_b2s]\n",
        "\n",
        "negative_dataset_b = [(rev_dict, \"Negative\")\n",
        "                     for rev_dict in n_toks_b2s]\n",
        "\n",
        "dataset_b = positive_dataset_b + negative_dataset_b\n",
        "\n",
        "random.shuffle(dataset_b)"
      ],
      "metadata": {
        "id": "tfCbDg7UQVJ7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = dataset_r + dataset_t + dataset_a + dataset_b\n",
        "random.shuffle(dataset)\n",
        "\n",
        "\n",
        "\n",
        "train_data = dataset[:12500]\n",
        "test_data = dataset[12500:]\n"
      ],
      "metadata": {
        "id": "gFbZoCl52LV0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import classify\n",
        "from nltk import SklearnClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "classifier = SklearnClassifier(RandomForestClassifier(), sparse=False).train(train_data)\n",
        "\n",
        "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gErKcdLohwNM",
        "outputId": "5c9e840e-b55b-474a-9e15-35df379ac918"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is: 0.9307123394316855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "oknaufJhyZXK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "2-oqoi0gy7Pp"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_save_name = 'classifier.pt'\n",
        "# path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "# torch.save(classifier, path)"
      ],
      "metadata": {
        "id": "eWUrIzp-y5o_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_2 = torch.load(path)"
      ],
      "metadata": {
        "id": "HcCzIQ5KykYK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corr = 0\n",
        "# incorr = 0\n",
        "# for i in pos_s2b:  \n",
        "#   if classifier.classify(dict([token, True] for token in remove_noise(word_tokenize(i)))) == 'Positive':\n",
        "#     corr += 1\n",
        "#   else:\n",
        "#     incorr += 1\n",
        "\n",
        "# pos_acc = (corr/(corr+incorr)) * 100"
      ],
      "metadata": {
        "id": "6lC269eKwFZa"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# false_pos_list = []\n",
        "\n",
        "# corr = 0\n",
        "# incorr = 0\n",
        "# for i in neg3_s2b:  \n",
        "#   if classifier.classify(dict([token, True] for token in remove_noise(word_tokenize(i)))) == 'Negative':\n",
        "#     corr += 1\n",
        "#   else:\n",
        "#     false_pos_list.append(i)\n",
        "#     incorr += 1\n",
        "\n",
        "# neg3_acc = (corr/(corr+incorr)) * 100"
      ],
      "metadata": {
        "id": "9wtJbUuywMEM"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "false_list = []\n",
        "correct_list = []\n",
        "\n",
        "corr = 0\n",
        "incorr = 0\n",
        "for i in validation_set:  \n",
        "  if classifier.classify(dict([token, True] for token in remove_noise(word_tokenize(i[1])))) == 'Negative' and i[0] == \"N\":\n",
        "    corr += 1\n",
        "    correct_list.append(i)\n",
        "\n",
        "  elif classifier.classify(dict([token, True] for token in remove_noise(word_tokenize(i[1])))) == 'Positive' and i[0] == \"P\":\n",
        "    corr += 1\n",
        "    correct_list.append(i)\n",
        "\n",
        "  else:\n",
        "    false_list.append(i)\n",
        "    incorr += 1\n",
        "\n",
        "acc = (corr/(corr+incorr)) * 100"
      ],
      "metadata": {
        "id": "w33IZZ83QxTU"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "metadata": {
        "id": "Tajl_gxTwT1q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db3f8cf3-0f77-4576-aad2-a9e3193b40fa"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93.65325077399382"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_tweet = \"all members are too friendly and helpful.\"\n",
        "\n",
        "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
        "\n",
        "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrIYtAD2g6vq",
        "outputId": "2ae7c3f3-50f5-4217-a3f3-bcb91f46a2e1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "false_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6jMVPo0FDCJ",
        "outputId": "a914c62d-ee07-475f-a385-612c8a3e1ddd"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['N',\n",
              "  'Staff were rude to me and constsntly asked me to start tasks right after being already set one. Annoyed because my previous stints went very well'],\n",
              " ['N',\n",
              "  'Rude and angry general manager, doesn’t think of you being his employees rather dealing like you are his servant.Terrible experience first time in a while working with stint. I will totally non-recommend.'],\n",
              " ['N', 'Staff are unprofessional and the environmemt is unhygienic'],\n",
              " ['N',\n",
              "  'Extremely innapropriate staff, grown men  were flirting with an 18 year old staff member'],\n",
              " ['N',\n",
              "  'Loved the customers, most employees are lackluster and instead of management sorting it out they want you to cover all their jobs, even when your new to the place, employees and managers kissing on the job, whilst im working and sweating'],\n",
              " ['N', 'It wasn’t a welcoming environment. Rude staff and extreme labour.'],\n",
              " ['N', 'Just did alot of cleaning'],\n",
              " ['N', 'Passive agressive'],\n",
              " ['P',\n",
              "  'To whom it may concern, \\n\\nI would like to thank you for having me at Terra Terra. The staff were all extremely welcoming, friendly and kind. However, I would like to call attention to the treatment I received from your staff member called Cynthia. I did briefly mention it after my shift to one of the managers, however I could see that it was a very busy period, so I did not want to take too much of his time to explain the situation. I felt very victimised and bullied by Cynthia during my shift, from the beginning until the end. \\n\\n* One instance is when Cynthia informed me that there weren’t any drinking glasses at the terrace and followed this with a patronising comment along the lines of: “We do have customers you know?” I feel like this was quite unfair because nobody had yet told me that there weren’t any drinking glasses left. I did later start to get into the habit of looking to see which glasses or utensils were low on stock, however this being my first shift it did naturally take a bit of time for me to do this. This is just one of the many snarky comments Cynthia said to me throughout the shift, which made me feel extremely disheartened. I voiced this to her and said that I didn’t think the extra comments were necessary. \\n* This being my first shift, I made a few mistakes. I accidentally brought up unpolished glasses and asked Cynthia whether there was a possibility that I could polish them at the terrace. She replied with something like “No. You do know that this is a classy restaurant? We can’t polish upstairs. Why would you not polish it downstairs?” I deem this response to be unnecessarily depreciating because a simple comment like “No, please do it downstairs.” would have been enough for me to get the message. I feel like Cynthia did not try to understand or empathise with the fact that I was new and would naturally make a few mistakes. Her corrections were very critical in nature and greatly dispirited me. \\n* On another occasion during the shift, a worker voiced that the drinking glasses required a restock and Cynthia loudly said around me “Yeah, I’ve told her multiple times.” This made me feel extremely humiliated. \\n* Cynthia would also loudly sigh and roll her eyes if I sometimes did not initially comprehend the orders she was giving me or asked for her to repeat herself. I feel like she was very impatient with me and showed little support, especially considering that this was my first shift with Terra Terra. \\n* After telling Cynthia discretely that I thought she was being a bit rude to me, she came to the kitchen to speak to me. At this point I informed her that at the end of my shift I would speak with the manager regarding her behaviour towards me, thinking that perhaps this would allow her to reconsider her behaviour towards me for the rest of the evening. She replied in a threatening tone: “You do know that we give you ratings too?” and attempted to escalate the situation by trying to make me go to the manager there and then. She said something like “Let’s go to the manager then.” I did not want to go, firstly because I felt extremely intimidated, and most importantly because this would pause me from doing my work which is what I was hired for. After I said once “I do not wish to discuss this here.” as I considered it extremely unprofessional and classless to argue in front of the rest of the staff, she continued to try to pressure me to go to the manager with her. This made me feel threatened and very embarrassed because many members of the kitchen staff, who I did not personally know, were nearby.\\n* About half an hour later she said to me “When you want to speak to the manager let me know”. I replied saying that I would prefer to have a private conversation with the manager to which she replied something like “Why would you want to do that? You can say what you want to say around me.”  which I felt was disrespecting my decision. \\n\\nTo conclude, I feel like I couldn’t work and perform to the best of my ability by virtue of the constant discouragement, belittlement and discourteousness I was subject to by Cynthia. I felt extremely bullied and her behaviour put me under a lot more stress than what I was already under by being at a new workplace during the very busy and hectic Saturday evening. \\n\\nI wish to end this feedback on a positive note. It is important for me to stress that the rest of your staff were extremely positive, courteous and helpful, and their good nature has meant that this has otherwise been my favourite stint to date. I again thank you for having me at Terra Terra.\\n\\nKind regards, \\n\\nMaria-Magdalena Jurga'],\n",
              " ['N',\n",
              "  \"Brasserie or bizarre adventure. I have already emailed details please let students know that they might ask you to sweep and work more than listed hours and you can't guarantee the behaviour and security.\"],\n",
              " ['N',\n",
              "  'I wish there should be two persons for kitchen porter. Work load was terrible that to on Saturday night people keep coming and Glasses keep coming.'],\n",
              " ['N',\n",
              "  'Gave me an extremely poor review despite working as hard as I could all night, asking frequently what more I could do to help, and recieving positive feedback at the time.'],\n",
              " ['N', 'Very rude & disrespectful manager. However some people were nice'],\n",
              " ['N',\n",
              "  'Nice restaurant but ruined by the horrible manager. She was extremely patronising, unfair and horrible'],\n",
              " ['N',\n",
              "  'The listed stint role was completely different from the actual role I was suppose to do. Very fast pace environment and management is not great.'],\n",
              " ['P',\n",
              "  'I’m leaving this rating not because I feel like it, but to improve the score of the tavern. I think the establishment should be more understanding of procedures. If I’m called from Stint to work there as a commis waiter please don’t make me work as a proper waiter taking orders and having my own section. Even though I have experience in this role and did it without complaint it wasn’t what I was supposed to do, so it lead to a lot of confusion on my part, being not sure if I should stay in the bar cleaning and polishing or go and do as I was told, since it basically was an extension of my trial shift. This was the main reason why it got so confusing, but as a student and as a potential new staff member there, I had little to no power to complain due to a fear of automatically being denied the position. (All of this can be double checked with an email I sent to studentsupport@stint.co on 18/12/021)'],\n",
              " ['N', 'Worst stint ever had'],\n",
              " ['N', 'Horrible place to work'],\n",
              " ['N', 'was never told I had to clean toilets thoroughly until I got there'],\n",
              " ['N',\n",
              "  'Horrible shift, with horrible manager who kept making comments about my breasts and made me so uncomfortable'],\n",
              " ['N',\n",
              "  'Manager was rude to other stint workers and tried to make then stay longer that their shift to get more cutlery ready gor a shift they both weren’ involved in - not impressed'],\n",
              " ['N',\n",
              "  'Terrible arrangement, they expect you to know their system of the bat with barely any example being shown to you, they talk badly to their runners and'],\n",
              " ['N',\n",
              "  'Worked non stop all night and received a three star review after staying an extra twenty minutes'],\n",
              " ['N',\n",
              "  'I was asked for this stint by a member of stint on message and i accepted the stint because she said i will be getting 40 pounds and now it shows only 35.6 !!!'],\n",
              " ['N', 'Very rude and unfriendly. Would not reccomend working here'],\n",
              " ['N',\n",
              "  'I stated my health conditions and disabilities however  they failed to give me a break.'],\n",
              " ['N',\n",
              "  'Managers and even staff were dismissive and unwelcoming. Very bosy and made you do all the work. Would not work there again unfortunately'],\n",
              " ['N',\n",
              "  'Doesn’t treat stint workers equally as employees. Terrible experience i reveived dozens of 5 start reviews before doing a stint at tenshi restaurant'],\n",
              " ['N',\n",
              "  'Was nothing to do, got treated poorly even though I got everything done quickly. Only there for free workers in my opinion'],\n",
              " ['N',\n",
              "  'It was tiring and stressful. The task that i was assigned was too much for one person and i was only provided help 1 hour before my Stint ended'],\n",
              " ['N',\n",
              "  'Terrible place with employees carrying a bad attitude against stint students'],\n",
              " ['N',\n",
              "  'Never workinworking here again. No actual management team yo support you.'],\n",
              " ['N',\n",
              "  'Terrible experience. You would not want to go to work at this place. Staff so mean and racist. They want you to take over their role while they chill. And does not appreciate your hard work.'],\n",
              " ['N', 'I got assaulted and they did absolutely nothing.'],\n",
              " ['N',\n",
              "  'I would like to give zero stars if i could. The staff are rude, not serious and make you do all the dirty work they dont want to do, so they are literally using us like toilet paper. Its a thumbs down for me👎🏾👎🏾'],\n",
              " ['N',\n",
              "  'didnt set instructions too clearly. Didnt make much effort to speak acknowledge or welcome me as a new worker'],\n",
              " ['N',\n",
              "  'Music was to loud in kitchen\\nExpected me to clean the same stairs 2  times which was really demotivating \\nInstructions in the kithces were unaudible because of music \\nStint had nothing about cleaning toilets'],\n",
              " ['N',\n",
              "  \"While most of the staff here are lovely the management do not care about the staff, especially the stints, and would rather the staff suffer in order to preserve an idealised image of the restaurant. The first thing the owner said to me on my first shift here was that most stints don't come back many times and now I see why.\"],\n",
              " ['N', 'Staff rude, underpayed for the amount of work'],\n",
              " ['P', 'got food :)'],\n",
              " ['N',\n",
              "  'Dangerous place to work at, all floors are slippery\\n\\nDrunk customers break glasses, if asked to move nicely.\\n\\nBouncers are not helping to stop the harassment.']]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# for i in correct_list:\n",
        "#   if i[0] == \"N\":\n",
        "#     print(i)\n"
      ],
      "metadata": {
        "id": "WLZWBHBdetH-"
      },
      "execution_count": 50,
      "outputs": []
    }
  ]
}